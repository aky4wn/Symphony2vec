{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNNs - Chromagrams, MFCC, Spectrograms\n",
    "\n",
    "Code References:\n",
    "- Data loading: https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "- Train and test functions adapted from discussion sections code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train_chroma = np.load(\"Data/train_chroma12.npy\")\n",
    "test_chroma = np.load(\"Data/test_chroma12.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_chroma12.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_chroma12.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 12, 431)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_chroma, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_chroma, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_chroma(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_chroma, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = (2,5),\n",
    "                              stride = (2,1), padding = (6,1))\n",
    "        self.conv2_bn1 = nn.BatchNorm2d(5)\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size = (2,4), stride = (2,2), padding = (6,2))\n",
    "        self.conv2_bn2 = nn.BatchNorm2d(10)\n",
    "        self.pool2 = nn.MaxPool2d((1, 2))\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size = (2,4), stride = (2,2), padding = (6,1))\n",
    "        self.conv2_bn3 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(10 * 12 * 27, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv2_bn1(self.conv1(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2_bn2(self.conv2(x))))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.conv2_bn3(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 10*12*27)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, test_loader, criterion, epoch, batch_size = 32):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.view(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f451247efb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_chroma() #.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6071\n",
      "Train(1)[50%]: Loss: 1.5458\n",
      "Test(1): Loss: 1.5153, Accuracy: 37.4284%\n",
      "Train(2)[0%]: Loss: 1.4474\n",
      "Train(2)[50%]: Loss: 1.4666\n",
      "Test(2): Loss: 1.5178, Accuracy: 36.9599%\n",
      "Train(3)[0%]: Loss: 1.3891\n",
      "Train(3)[50%]: Loss: 1.4250\n",
      "Test(3): Loss: 1.5281, Accuracy: 36.4914%\n",
      "Train(4)[0%]: Loss: 1.4568\n",
      "Train(4)[50%]: Loss: 1.3863\n",
      "Test(4): Loss: 1.5255, Accuracy: 37.4284%\n",
      "Train(5)[0%]: Loss: 1.2572\n",
      "Train(5)[50%]: Loss: 1.3771\n",
      "Test(5): Loss: 1.5244, Accuracy: 37.5846%\n",
      "Train(6)[0%]: Loss: 1.3005\n",
      "Train(6)[50%]: Loss: 1.3324\n",
      "Test(6): Loss: 1.5265, Accuracy: 37.6887%\n",
      "Train(7)[0%]: Loss: 1.2221\n",
      "Train(7)[50%]: Loss: 1.3181\n",
      "Test(7): Loss: 1.5247, Accuracy: 38.0271%\n",
      "Train(8)[0%]: Loss: 1.3349\n",
      "Train(8)[50%]: Loss: 1.2981\n",
      "Test(8): Loss: 1.5316, Accuracy: 37.5846%\n",
      "Train(9)[0%]: Loss: 1.2951\n",
      "Train(9)[50%]: Loss: 1.2837\n",
      "Test(9): Loss: 1.5334, Accuracy: 36.8037%\n",
      "Train(10)[0%]: Loss: 1.4141\n",
      "Train(10)[50%]: Loss: 1.2626\n",
      "Test(10): Loss: 1.5368, Accuracy: 36.8558%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train_mfcc = np.load(\"Data/train_mfcc13.npy\")\n",
    "test_mfcc = np.load(\"Data/test_mfcc13.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_mfcc13.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_mfcc13.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_mfcc, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_mfcc, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_mfcc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_mfcc, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = (4,5),\n",
    "                              stride = (1,1), padding = (1,1))\n",
    "        self.conv2_bn1 = nn.BatchNorm2d(5)\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size = (2,4), stride = (2,2), padding = (6,2))\n",
    "        self.conv2_bn2 = nn.BatchNorm2d(10)\n",
    "        self.pool2 = nn.MaxPool2d((1, 2))\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size = (2,4), stride = (2,2), padding = (6,1))\n",
    "        self.conv2_bn3 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(10 * 12 * 27, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv2_bn1(self.conv1(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2_bn2(self.conv2(x))))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.conv2_bn3(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 10*12*27)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_mfcc() #.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6042\n",
      "Train(1)[50%]: Loss: 1.5356\n",
      "Test(1): Loss: 1.4782, Accuracy: 41.9053%\n",
      "Train(2)[0%]: Loss: 1.4521\n",
      "Train(2)[50%]: Loss: 1.3791\n",
      "Test(2): Loss: 1.4133, Accuracy: 47.8917%\n",
      "Train(3)[0%]: Loss: 1.3203\n",
      "Train(3)[50%]: Loss: 1.3596\n",
      "Test(3): Loss: 1.4129, Accuracy: 48.5164%\n",
      "Train(4)[0%]: Loss: 1.3375\n",
      "Train(4)[50%]: Loss: 1.3264\n",
      "Test(4): Loss: 1.4032, Accuracy: 49.4014%\n",
      "Train(5)[0%]: Loss: 1.2834\n",
      "Train(5)[50%]: Loss: 1.2996\n",
      "Test(5): Loss: 1.3689, Accuracy: 53.8261%\n",
      "Train(6)[0%]: Loss: 1.3184\n",
      "Train(6)[50%]: Loss: 1.2872\n",
      "Test(6): Loss: 1.3447, Accuracy: 55.4399%\n",
      "Train(7)[0%]: Loss: 1.3113\n",
      "Train(7)[50%]: Loss: 1.2583\n",
      "Test(7): Loss: 1.3785, Accuracy: 52.5508%\n",
      "Train(8)[0%]: Loss: 1.2779\n",
      "Train(8)[50%]: Loss: 1.2386\n",
      "Test(8): Loss: 1.3696, Accuracy: 53.2535%\n",
      "Train(9)[0%]: Loss: 1.1491\n",
      "Train(9)[50%]: Loss: 1.1988\n",
      "Test(9): Loss: 1.3400, Accuracy: 55.8824%\n",
      "Train(10)[0%]: Loss: 1.1839\n",
      "Train(10)[50%]: Loss: 1.1960\n",
      "Test(10): Loss: 1.3824, Accuracy: 52.8371%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_spec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_spec, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = (4,3),\n",
    "                              stride = (2,2), padding = (1,1))\n",
    "        self.conv2_bn1 = nn.BatchNorm2d(5)\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.conv2_bn2 = nn.BatchNorm2d(10)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size = (3,4), stride = 2, padding = 1)\n",
    "        self.conv2_bn3 = nn.BatchNorm2d(10)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(10 * 4 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool1(F.relu(self.conv2_bn1(self.conv1(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool2(F.relu(self.conv2_bn2(self.conv2(x))))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.conv2_bn3(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 10*4*13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-1.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-1.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-1.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-1.npy\")\n",
    "\n",
    "## Load new data\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-2.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-2.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-2.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-2.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2147, 64, 431)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_spec() #.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6109\n",
      "Train(1)[50%]: Loss: 1.5690\n",
      "Test(1): Loss: 1.5301, Accuracy: 37.0611%\n",
      "Train(2)[0%]: Loss: 1.4488\n",
      "Train(2)[50%]: Loss: 1.4555\n",
      "Test(2): Loss: 1.5499, Accuracy: 35.0455%\n",
      "Train(3)[0%]: Loss: 1.4551\n",
      "Train(3)[50%]: Loss: 1.3914\n",
      "Test(3): Loss: 1.4649, Accuracy: 44.9935%\n",
      "Train(4)[0%]: Loss: 1.3724\n",
      "Train(4)[50%]: Loss: 1.3235\n",
      "Test(4): Loss: 1.4490, Accuracy: 46.7490%\n",
      "Train(5)[0%]: Loss: 1.3249\n",
      "Train(5)[50%]: Loss: 1.3092\n",
      "Test(5): Loss: 1.4141, Accuracy: 50.5202%\n",
      "Train(6)[0%]: Loss: 1.1366\n",
      "Train(6)[50%]: Loss: 1.2674\n",
      "Test(6): Loss: 1.4727, Accuracy: 45.5137%\n",
      "Train(7)[0%]: Loss: 1.2889\n",
      "Train(7)[50%]: Loss: 1.2346\n",
      "Test(7): Loss: 1.4158, Accuracy: 50.1300%\n",
      "Train(8)[0%]: Loss: 1.2681\n",
      "Train(8)[50%]: Loss: 1.2169\n",
      "Test(8): Loss: 1.4544, Accuracy: 47.0091%\n",
      "Train(9)[0%]: Loss: 1.1763\n",
      "Train(9)[50%]: Loss: 1.1921\n",
      "Test(9): Loss: 1.4191, Accuracy: 50.4551%\n",
      "Train(10)[0%]: Loss: 1.2738\n",
      "Train(10)[50%]: Loss: 1.1809\n",
      "Test(10): Loss: 1.4008, Accuracy: 52.5358%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.3467\n",
      "Train(1)[50%]: Loss: 1.3275\n",
      "Test(1): Loss: 1.3622, Accuracy: 53.2552%\n",
      "Train(2)[0%]: Loss: 1.2420\n",
      "Train(2)[50%]: Loss: 1.2599\n",
      "Test(2): Loss: 1.3596, Accuracy: 53.7760%\n",
      "Train(3)[0%]: Loss: 1.1337\n",
      "Train(3)[50%]: Loss: 1.2373\n",
      "Test(3): Loss: 1.3748, Accuracy: 52.0833%\n",
      "Train(4)[0%]: Loss: 1.2189\n",
      "Train(4)[50%]: Loss: 1.2459\n",
      "Test(4): Loss: 1.3503, Accuracy: 54.5573%\n",
      "Train(5)[0%]: Loss: 1.2015\n",
      "Train(5)[50%]: Loss: 1.2352\n",
      "Test(5): Loss: 1.3461, Accuracy: 55.0130%\n",
      "Train(6)[0%]: Loss: 1.2104\n",
      "Train(6)[50%]: Loss: 1.2025\n",
      "Test(6): Loss: 1.4152, Accuracy: 48.6328%\n",
      "Train(7)[0%]: Loss: 1.1580\n",
      "Train(7)[50%]: Loss: 1.2129\n",
      "Test(7): Loss: 1.3786, Accuracy: 51.8229%\n",
      "Train(8)[0%]: Loss: 1.2848\n",
      "Train(8)[50%]: Loss: 1.1851\n",
      "Test(8): Loss: 1.3484, Accuracy: 54.4922%\n",
      "Train(9)[0%]: Loss: 1.2512\n",
      "Train(9)[50%]: Loss: 1.1884\n",
      "Test(9): Loss: 1.3381, Accuracy: 56.1198%\n",
      "Train(10)[0%]: Loss: 1.0830\n",
      "Train(10)[50%]: Loss: 1.1522\n",
      "Test(10): Loss: 1.3441, Accuracy: 55.3385%\n"
     ]
    }
   ],
   "source": [
    "## Load new data\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-3.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-3.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-3.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-3.npy\")\n",
    "\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-4.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-4.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-4.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-4.npy\")\n",
    "\n",
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.2747\n",
      "Train(1)[50%]: Loss: 1.3332\n",
      "Test(1): Loss: 1.3427, Accuracy: 55.0781%\n",
      "Train(2)[0%]: Loss: 1.4820\n",
      "Train(2)[50%]: Loss: 1.2824\n",
      "Test(2): Loss: 1.3350, Accuracy: 56.5104%\n",
      "Train(3)[0%]: Loss: 1.1846\n",
      "Train(3)[50%]: Loss: 1.2499\n",
      "Test(3): Loss: 1.3890, Accuracy: 51.0417%\n",
      "Train(4)[0%]: Loss: 1.2994\n",
      "Train(4)[50%]: Loss: 1.2902\n",
      "Test(4): Loss: 1.4046, Accuracy: 49.7396%\n",
      "Train(5)[0%]: Loss: 1.4702\n",
      "Train(5)[50%]: Loss: 1.2386\n",
      "Test(5): Loss: 1.3667, Accuracy: 51.3021%\n",
      "Train(6)[0%]: Loss: 1.1716\n",
      "Train(6)[50%]: Loss: 1.2097\n",
      "Test(6): Loss: 1.3462, Accuracy: 54.4271%\n",
      "Train(7)[0%]: Loss: 1.1275\n",
      "Train(7)[50%]: Loss: 1.1982\n",
      "Test(7): Loss: 1.3724, Accuracy: 52.6042%\n",
      "Train(8)[0%]: Loss: 1.2099\n",
      "Train(8)[50%]: Loss: 1.1538\n",
      "Test(8): Loss: 1.3562, Accuracy: 53.7760%\n",
      "Train(9)[0%]: Loss: 1.2043\n",
      "Train(9)[50%]: Loss: 1.1286\n",
      "Test(9): Loss: 1.3844, Accuracy: 51.0417%\n",
      "Train(10)[0%]: Loss: 1.1110\n",
      "Train(10)[50%]: Loss: 1.1205\n",
      "Test(10): Loss: 1.3377, Accuracy: 55.7292%\n"
     ]
    }
   ],
   "source": [
    "## Load new data\n",
    "train_stft = np.load(\"Data/train_stft-dB-5.npy\", allow_pickle = True)\n",
    "test_stft = np.load(\"Data/test_stft-dB-5.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_stft-dB-5.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_stft-dB-5.npy\")\n",
    "\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
