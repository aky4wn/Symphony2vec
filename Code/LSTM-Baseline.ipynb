{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Baseline - Chromagram, MFCCs, Spectrogram\n",
    "\n",
    "Code References:\n",
    "- Data loading: https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "- Train and test functions adapted from discussion sections code\n",
    "- LSTM Code adapted from https://discuss.pytorch.org/t/example-of-many-to-one-lstm/1728/2\n",
    "\n",
    "LSTM has two hidden layers and 5 dimensional hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train_chroma = np.load(\"Data/train_chroma12.npy\")\n",
    "test_chroma = np.load(\"Data/test_chroma12.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_chroma12.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_chroma12.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 12, 431)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Need to split sequences, currently too long\n",
    "train_chroma.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainC = np.concatenate(np.array_split(train_chroma, 12, axis = 2)[:-1])\n",
    "train_labels = np.tile(train_labels, 11)\n",
    "\n",
    "testC = np.concatenate(np.array_split(test_chroma, 12, axis = 2)[:-1])\n",
    "test_labels = np.tile(test_labels, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59026, 12, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1, ..., 2, 4, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(trainC, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(testC, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code adapted from https://discuss.pytorch.org/t/example-of-many-to-one-lstm/1728/2\n",
    "\n",
    "classes_no = 5\n",
    "in_size = 12\n",
    "\n",
    "model = nn.LSTM(in_size, classes_no, 2).float()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters()) #lr=0.0001, weight_decay=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_size = data.shape[0]\n",
    "        time_steps = data.shape[2]\n",
    "        in_size = data.shape[1]\n",
    "        \n",
    "        \n",
    "        input_seq = data.view(time_steps, batch_size, in_size)\n",
    "        \n",
    "        output_seq, _ = model(input_seq.float())\n",
    "        last_output = output_seq[-1]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = criterion(last_output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, test_loader, criterion, epoch, batch_size = 32):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            batch_size = data.shape[0]\n",
    "            time_steps = data.shape[2]\n",
    "            in_size = data.shape[1]\n",
    "        \n",
    "        \n",
    "            input_seq = data.view(time_steps, batch_size, in_size)\n",
    "        \n",
    "            output_seq, _ = model(input_seq.float())\n",
    "            last_output = output_seq[-1]\n",
    "            \n",
    "            test_loss += criterion(last_output, target).item() # sum up batch loss\n",
    "            pred = last_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6192\n",
      "Train(1)[50%]: Loss: 1.5803\n",
      "Train(1)[100%]: Loss: 1.5783\n",
      "Test(1): Loss: 1.0909, Accuracy: 25.1124%\n",
      "Train(2)[0%]: Loss: 1.5080\n",
      "Train(2)[50%]: Loss: 1.5770\n",
      "Train(2)[100%]: Loss: 1.5756\n",
      "Test(2): Loss: 1.0895, Accuracy: 25.1195%\n",
      "Train(3)[0%]: Loss: 1.5614\n",
      "Train(3)[50%]: Loss: 1.5751\n",
      "Train(3)[100%]: Loss: 1.5757\n",
      "Test(3): Loss: 1.0905, Accuracy: 25.1124%\n",
      "Train(4)[0%]: Loss: 1.4916\n",
      "Train(4)[50%]: Loss: 1.5764\n",
      "Train(4)[100%]: Loss: 1.5755\n",
      "Test(4): Loss: 1.0897, Accuracy: 25.1219%\n",
      "Train(5)[0%]: Loss: 1.5889\n",
      "Train(5)[50%]: Loss: 1.5754\n",
      "Train(5)[100%]: Loss: 1.5755\n",
      "Test(5): Loss: 1.0903, Accuracy: 25.1337%\n",
      "Train(6)[0%]: Loss: 1.5464\n",
      "Train(6)[50%]: Loss: 1.5747\n",
      "Train(6)[100%]: Loss: 1.5754\n",
      "Test(6): Loss: 1.0900, Accuracy: 25.1171%\n",
      "Train(7)[0%]: Loss: 1.5618\n",
      "Train(7)[50%]: Loss: 1.5761\n",
      "Train(7)[100%]: Loss: 1.5754\n",
      "Test(7): Loss: 1.0907, Accuracy: 25.1171%\n",
      "Train(8)[0%]: Loss: 1.6087\n",
      "Train(8)[50%]: Loss: 1.5740\n",
      "Train(8)[100%]: Loss: 1.5754\n",
      "Test(8): Loss: 1.0899, Accuracy: 25.1171%\n",
      "Train(9)[0%]: Loss: 1.4993\n",
      "Train(9)[50%]: Loss: 1.5751\n",
      "Train(9)[100%]: Loss: 1.5754\n",
      "Test(9): Loss: 1.0898, Accuracy: 25.1171%\n",
      "Train(10)[0%]: Loss: 1.5933\n",
      "Train(10)[50%]: Loss: 1.5755\n",
      "Train(10)[100%]: Loss: 1.5753\n",
      "Test(10): Loss: 1.0893, Accuracy: 25.1171%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 13, 431)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load data\n",
    "train_mfcc = np.load(\"Data/train_mfcc13.npy\")\n",
    "test_mfcc = np.load(\"Data/test_mfcc13.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_mfcc13.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_mfcc13.npy\")\n",
    "## Need to split sequences, currently too long\n",
    "train_mfcc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainM = np.concatenate(np.array_split(train_mfcc, 12, axis = 2)[:-1])\n",
    "train_labels = np.tile(train_labels, 11)\n",
    "\n",
    "testM = np.concatenate(np.array_split(test_mfcc, 12, axis = 2)[:-1])\n",
    "test_labels = np.tile(test_labels, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59026, 13, 36)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(trainM, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(testM, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_no = 5\n",
    "in_size = 13\n",
    "\n",
    "model = nn.LSTM(in_size, classes_no, 2).float()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) #lr=0.0001, weight_decay=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6423\n",
      "Train(1)[50%]: Loss: 1.6250\n",
      "Train(1)[100%]: Loss: 1.6147\n",
      "Test(1): Loss: 1.0978, Accuracy: 25.1195%\n",
      "Train(2)[0%]: Loss: 1.6001\n",
      "Train(2)[50%]: Loss: 1.5862\n",
      "Train(2)[100%]: Loss: 1.5838\n",
      "Test(2): Loss: 1.0914, Accuracy: 25.1171%\n",
      "Train(3)[0%]: Loss: 1.5813\n",
      "Train(3)[50%]: Loss: 1.5788\n",
      "Train(3)[100%]: Loss: 1.5774\n",
      "Test(3): Loss: 1.0901, Accuracy: 25.1171%\n",
      "Train(4)[0%]: Loss: 1.5414\n",
      "Train(4)[50%]: Loss: 1.5760\n",
      "Train(4)[100%]: Loss: 1.5756\n",
      "Test(4): Loss: 1.0901, Accuracy: 25.1171%\n",
      "Train(5)[0%]: Loss: 1.6211\n",
      "Train(5)[50%]: Loss: 1.5755\n",
      "Train(5)[100%]: Loss: 1.5754\n",
      "Test(5): Loss: 1.0903, Accuracy: 25.1171%\n",
      "Train(6)[0%]: Loss: 1.6749\n",
      "Train(6)[50%]: Loss: 1.5751\n",
      "Train(6)[100%]: Loss: 1.5753\n",
      "Test(6): Loss: 1.0903, Accuracy: 25.1171%\n",
      "Train(7)[0%]: Loss: 1.5018\n",
      "Train(7)[50%]: Loss: 1.5766\n",
      "Train(7)[100%]: Loss: 1.5753\n",
      "Test(7): Loss: 1.0904, Accuracy: 25.1171%\n",
      "Train(8)[0%]: Loss: 1.5397\n",
      "Train(8)[50%]: Loss: 1.5742\n",
      "Train(8)[100%]: Loss: 1.5753\n",
      "Test(8): Loss: 1.0902, Accuracy: 25.1171%\n",
      "Train(9)[0%]: Loss: 1.5346\n",
      "Train(9)[50%]: Loss: 1.5735\n",
      "Train(9)[100%]: Loss: 1.5752\n",
      "Test(9): Loss: 1.0903, Accuracy: 25.1171%\n",
      "Train(10)[0%]: Loss: 1.5192\n",
      "Train(10)[50%]: Loss: 1.5761\n",
      "Train(10)[100%]: Loss: 1.5752\n",
      "Test(10): Loss: 1.0903, Accuracy: 25.1171%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2147, 64, 431)\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-1.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-1.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-1.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-1.npy\")\n",
    "\n",
    "## Load new data\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-2.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-2.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-2.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-2.npy\")\n",
    "\n",
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))\n",
    "\n",
    "print(train_stft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainS = np.concatenate(np.array_split(train_stft, 12, axis = 2)[:-1])\n",
    "train_labels = np.tile(train_labels, 11)\n",
    "\n",
    "testS = np.concatenate(np.array_split(test_stft, 12, axis = 2)[:-1])\n",
    "test_labels = np.tile(test_labels, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23617, 64, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(trainS, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(testS, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_no = 5\n",
    "in_size = 64\n",
    "\n",
    "model = nn.LSTM(in_size, classes_no, 2).float()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6520\n",
      "Train(1)[50%]: Loss: 1.5988\n",
      "Train(1)[100%]: Loss: 1.5941\n",
      "Test(1): Loss: 1.0977, Accuracy: 23.7262%\n",
      "Train(2)[0%]: Loss: 1.5751\n",
      "Train(2)[50%]: Loss: 1.5844\n",
      "Train(2)[100%]: Loss: 1.5822\n",
      "Test(2): Loss: 1.0951, Accuracy: 24.3468%\n",
      "Train(3)[0%]: Loss: 1.5766\n",
      "Train(3)[50%]: Loss: 1.5806\n",
      "Train(3)[100%]: Loss: 1.5778\n",
      "Test(3): Loss: 1.0946, Accuracy: 24.3409%\n",
      "Train(4)[0%]: Loss: 1.4484\n",
      "Train(4)[50%]: Loss: 1.5803\n",
      "Train(4)[100%]: Loss: 1.5768\n",
      "Test(4): Loss: 1.0944, Accuracy: 24.3646%\n",
      "Train(5)[0%]: Loss: 1.5410\n",
      "Train(5)[50%]: Loss: 1.5792\n",
      "Train(5)[100%]: Loss: 1.5761\n",
      "Test(5): Loss: 1.0943, Accuracy: 24.3705%\n",
      "Train(6)[0%]: Loss: 1.5542\n",
      "Train(6)[50%]: Loss: 1.5747\n",
      "Train(6)[100%]: Loss: 1.5763\n",
      "Test(6): Loss: 1.0942, Accuracy: 24.3705%\n",
      "Train(7)[0%]: Loss: 1.5921\n",
      "Train(7)[50%]: Loss: 1.5759\n",
      "Train(7)[100%]: Loss: 1.5752\n",
      "Test(7): Loss: 1.0941, Accuracy: 24.3587%\n",
      "Train(8)[0%]: Loss: 1.5238\n",
      "Train(8)[50%]: Loss: 1.5735\n",
      "Train(8)[100%]: Loss: 1.5752\n",
      "Test(8): Loss: 1.0940, Accuracy: 24.3587%\n",
      "Train(9)[0%]: Loss: 1.5300\n",
      "Train(9)[50%]: Loss: 1.5708\n",
      "Train(9)[100%]: Loss: 1.5748\n",
      "Test(9): Loss: 1.0940, Accuracy: 24.3587%\n",
      "Train(10)[0%]: Loss: 1.5101\n",
      "Train(10)[50%]: Loss: 1.5733\n",
      "Train(10)[100%]: Loss: 1.5749\n",
      "Test(10): Loss: 1.0939, Accuracy: 24.3705%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2146, 64, 431)\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-3.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-3.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-3.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-3.npy\")\n",
    "\n",
    "## Load new data\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-4.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-4.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-4.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-4.npy\")\n",
    "\n",
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))\n",
    "\n",
    "print(train_stft.shape)\n",
    "\n",
    "trainS = np.concatenate(np.array_split(train_stft, 12, axis = 2)[:-1])\n",
    "train_labels = np.tile(train_labels, 11)\n",
    "\n",
    "testS = np.concatenate(np.array_split(test_stft, 12, axis = 2)[:-1])\n",
    "test_labels = np.tile(test_labels, 11)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(trainS, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(testS, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.5894\n",
      "Train(1)[50%]: Loss: 1.5757\n",
      "Test(1): Loss: 1.5871, Accuracy: 25.5741%\n",
      "Train(2)[0%]: Loss: 1.5440\n",
      "Train(2)[50%]: Loss: 1.5747\n",
      "Test(2): Loss: 1.5874, Accuracy: 25.4202%\n",
      "Train(3)[0%]: Loss: 1.7034\n",
      "Train(3)[50%]: Loss: 1.5729\n",
      "Test(3): Loss: 1.5870, Accuracy: 25.3729%\n",
      "Train(4)[0%]: Loss: 1.5665\n",
      "Train(4)[50%]: Loss: 1.5739\n",
      "Test(4): Loss: 1.5870, Accuracy: 25.3137%\n",
      "Train(5)[0%]: Loss: 1.5195\n",
      "Train(5)[50%]: Loss: 1.5734\n",
      "Test(5): Loss: 1.5876, Accuracy: 25.3965%\n",
      "Train(6)[0%]: Loss: 1.6145\n",
      "Train(6)[50%]: Loss: 1.5719\n",
      "Test(6): Loss: 1.5872, Accuracy: 25.3610%\n",
      "Train(7)[0%]: Loss: 1.5638\n",
      "Train(7)[50%]: Loss: 1.5712\n",
      "Test(7): Loss: 1.5869, Accuracy: 25.2545%\n",
      "Train(8)[0%]: Loss: 1.6528\n",
      "Train(8)[50%]: Loss: 1.5703\n",
      "Test(8): Loss: 1.5873, Accuracy: 25.3018%\n",
      "Train(9)[0%]: Loss: 1.5396\n",
      "Train(9)[50%]: Loss: 1.5697\n",
      "Test(9): Loss: 1.5871, Accuracy: 25.3374%\n",
      "Train(10)[0%]: Loss: 1.5201\n",
      "Train(10)[50%]: Loss: 1.5731\n",
      "Test(10): Loss: 1.5875, Accuracy: 25.1361%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073, 64, 431)\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "\n",
    "train_stft = np.load(\"Data/train_stft-dB-5.npy\", allow_pickle = True)\n",
    "test_stft = np.load(\"Data/test_stft-dB-5.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_stft-dB-5.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_stft-dB-5.npy\")\n",
    "\n",
    "print(train_stft.shape)\n",
    "\n",
    "trainS = np.concatenate(np.array_split(train_stft, 12, axis = 2)[:-1])\n",
    "train_labels = np.tile(train_labels, 11)\n",
    "\n",
    "testS = np.concatenate(np.array_split(test_stft, 12, axis = 2)[:-1])\n",
    "test_labels = np.tile(test_labels, 11)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(trainS, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(testS, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.5610\n",
      "Train(1)[50%]: Loss: 1.5867\n",
      "Train(1)[100%]: Loss: 1.5830\n",
      "Test(1): Loss: 1.5795, Accuracy: 25.2486%\n",
      "Train(2)[0%]: Loss: 1.5371\n",
      "Train(2)[50%]: Loss: 1.5864\n",
      "Train(2)[100%]: Loss: 1.5824\n",
      "Test(2): Loss: 1.5798, Accuracy: 25.2723%\n",
      "Train(3)[0%]: Loss: 1.6640\n",
      "Train(3)[50%]: Loss: 1.5838\n",
      "Train(3)[100%]: Loss: 1.5817\n",
      "Test(3): Loss: 1.5802, Accuracy: 25.2131%\n",
      "Train(4)[0%]: Loss: 1.5385\n",
      "Train(4)[50%]: Loss: 1.5839\n",
      "Train(4)[100%]: Loss: 1.5822\n",
      "Test(4): Loss: 1.5804, Accuracy: 25.2249%\n",
      "Train(5)[0%]: Loss: 1.5474\n",
      "Train(5)[50%]: Loss: 1.5849\n",
      "Train(5)[100%]: Loss: 1.5820\n",
      "Test(5): Loss: 1.5806, Accuracy: 25.2723%\n",
      "Train(6)[0%]: Loss: 1.5745\n",
      "Train(6)[50%]: Loss: 1.5820\n",
      "Train(6)[100%]: Loss: 1.5822\n",
      "Test(6): Loss: 1.5809, Accuracy: 25.2249%\n",
      "Train(7)[0%]: Loss: 1.6644\n",
      "Train(7)[50%]: Loss: 1.5820\n",
      "Train(7)[100%]: Loss: 1.5818\n",
      "Test(7): Loss: 1.5809, Accuracy: 25.2131%\n",
      "Train(8)[0%]: Loss: 1.6928\n",
      "Train(8)[50%]: Loss: 1.5825\n",
      "Train(8)[100%]: Loss: 1.5817\n",
      "Test(8): Loss: 1.5810, Accuracy: 25.0947%\n",
      "Train(9)[0%]: Loss: 1.6159\n",
      "Train(9)[50%]: Loss: 1.5814\n",
      "Train(9)[100%]: Loss: 1.5819\n",
      "Test(9): Loss: 1.5812, Accuracy: 25.2604%\n",
      "Train(10)[0%]: Loss: 1.5125\n",
      "Train(10)[50%]: Loss: 1.5819\n",
      "Train(10)[100%]: Loss: 1.5820\n",
      "Test(10): Loss: 1.5811, Accuracy: 25.2131%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
