{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines - Feed Forward for Different Representations\n",
    "\n",
    "Code References:\n",
    "- Data loading: https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "- Train and test functions adapted from discussion sections code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron - 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_channels, out_features= 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features= out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.softmax(self.fc2(x), dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train_chroma = np.load(\"Data/train_chroma12.npy\")\n",
    "test_chroma = np.load(\"Data/test_chroma12.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_chroma12.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_chroma12.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 12, 431)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3842, 12, 431)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003052163 0.9994745\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_chroma), np.std(train_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_chroma, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_chroma, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 12, 431)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(data.shape[0], data.shape[1]*data.shape[2])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, test_loader, criterion, epoch, batch_size = 32):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.view(data.shape[0], data.shape[1]*data.shape[2])\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}, Accuracy: {:.4f}%'.format(\n",
    "        epoch, test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f38295fb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# device = torch.device(device)\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_chroma.shape[1]*train_chroma.shape[2]\n",
    "out_channels = 5\n",
    "model = perceptron(in_channels, out_channels) #.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6113\n",
      "Train(1)[50%]: Loss: 1.4828\n",
      "Test(1): Loss: 1.5031, Accuracy: 38.4956%\n",
      "Train(2)[0%]: Loss: 1.3561\n",
      "Train(2)[50%]: Loss: 1.3047\n",
      "Test(2): Loss: 1.5130, Accuracy: 38.4695%\n",
      "Train(3)[0%]: Loss: 1.3924\n",
      "Train(3)[50%]: Loss: 1.2124\n",
      "Test(3): Loss: 1.5128, Accuracy: 37.0901%\n",
      "Train(4)[0%]: Loss: 1.2954\n",
      "Train(4)[50%]: Loss: 1.1342\n",
      "Test(4): Loss: 1.5219, Accuracy: 36.7257%\n",
      "Train(5)[0%]: Loss: 1.2120\n",
      "Train(5)[50%]: Loss: 1.0807\n",
      "Test(5): Loss: 1.5254, Accuracy: 36.2051%\n",
      "Train(6)[0%]: Loss: 0.9814\n",
      "Train(6)[50%]: Loss: 1.0395\n",
      "Test(6): Loss: 1.5366, Accuracy: 35.1119%\n",
      "Train(7)[0%]: Loss: 1.1278\n",
      "Train(7)[50%]: Loss: 1.0150\n",
      "Test(7): Loss: 1.5362, Accuracy: 35.2421%\n",
      "Train(8)[0%]: Loss: 0.9940\n",
      "Train(8)[50%]: Loss: 0.9878\n",
      "Test(8): Loss: 1.5396, Accuracy: 34.8777%\n",
      "Train(9)[0%]: Loss: 0.9591\n",
      "Train(9)[50%]: Loss: 0.9760\n",
      "Test(9): Loss: 1.5387, Accuracy: 34.8256%\n",
      "Train(10)[0%]: Loss: 1.0769\n",
      "Train(10)[50%]: Loss: 0.9633\n",
      "Test(10): Loss: 1.5414, Accuracy: 34.3571%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train_mfcc = np.load(\"Data/train_mfcc13.npy\")\n",
    "test_mfcc = np.load(\"Data/test_mfcc13.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_mfcc13.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_mfcc13.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set_mfcc = MyDataset(train_mfcc, train_labels)\n",
    "train_loader_mfcc = DataLoader(train_set_mfcc, batch_size=bs, shuffle=True)\n",
    "test_set_mfcc = MyDataset(test_mfcc, test_labels)\n",
    "test_loader_mfcc = DataLoader(test_set_mfcc, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_mfcc.shape[1]*train_mfcc.shape[2]\n",
    "out_channels = 5\n",
    "model = perceptron(in_channels, out_channels) #.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6064\n",
      "Train(1)[50%]: Loss: 1.4532\n",
      "Test(1): Loss: 1.4213, Accuracy: 47.1629%\n",
      "Train(2)[0%]: Loss: 1.2893\n",
      "Train(2)[50%]: Loss: 1.3162\n",
      "Test(2): Loss: 1.3757, Accuracy: 55.5440%\n",
      "Train(3)[0%]: Loss: 1.2204\n",
      "Train(3)[50%]: Loss: 1.2551\n",
      "Test(3): Loss: 1.4045, Accuracy: 50.0260%\n",
      "Train(4)[0%]: Loss: 1.2398\n",
      "Train(4)[50%]: Loss: 1.2168\n",
      "Test(4): Loss: 1.3956, Accuracy: 50.1562%\n",
      "Train(5)[0%]: Loss: 1.2242\n",
      "Train(5)[50%]: Loss: 1.1859\n",
      "Test(5): Loss: 1.3698, Accuracy: 54.9714%\n",
      "Train(6)[0%]: Loss: 1.1897\n",
      "Train(6)[50%]: Loss: 1.1575\n",
      "Test(6): Loss: 1.3736, Accuracy: 53.9563%\n",
      "Train(7)[0%]: Loss: 1.2222\n",
      "Train(7)[50%]: Loss: 1.1319\n",
      "Test(7): Loss: 1.3672, Accuracy: 54.1645%\n",
      "Train(8)[0%]: Loss: 1.2113\n",
      "Train(8)[50%]: Loss: 1.1038\n",
      "Test(8): Loss: 1.4226, Accuracy: 47.8917%\n",
      "Train(9)[0%]: Loss: 1.0940\n",
      "Train(9)[50%]: Loss: 1.0970\n",
      "Test(9): Loss: 1.3703, Accuracy: 53.9823%\n",
      "Train(10)[0%]: Loss: 1.1379\n",
      "Train(10)[50%]: Loss: 1.0922\n",
      "Test(10): Loss: 1.3670, Accuracy: 53.9823%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader_mfcc, criterion, optimizer, epoch)\n",
    "    test(model, test_loader_mfcc, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward - Examine Impact of Vertical + Horizontal Structure Loss\n",
    "\n",
    "- 4 hidden layers\n",
    "- each has 1024 hidden units\n",
    "- batch normalization and weight decay\n",
    "- relu for each activation\n",
    "- softmax output and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(feed_forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_channels, out_features= 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features= 512)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features= 256)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=256)\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features= 128)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=128)\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features= 64)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=64)\n",
    "        self.fc6 = nn.Linear(in_features=64, out_features=out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "#         x = self.fc4(x)\n",
    "        x = F.softmax(self.fc6(x), dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chromagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_chroma.shape[1]*train_chroma.shape[2]\n",
    "out_channels = 5\n",
    "model = feed_forward(in_channels, out_channels) #.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6074\n",
      "Train(1)[50%]: Loss: 1.5573\n",
      "Test(1): Loss: 1.5372, Accuracy: 39.4066%\n",
      "Train(2)[0%]: Loss: 1.3934\n",
      "Train(2)[50%]: Loss: 1.3612\n",
      "Test(2): Loss: 1.5252, Accuracy: 40.7340%\n",
      "Train(3)[0%]: Loss: 1.2151\n",
      "Train(3)[50%]: Loss: 1.2166\n",
      "Test(3): Loss: 1.5157, Accuracy: 40.5258%\n",
      "Train(4)[0%]: Loss: 1.1171\n",
      "Train(4)[50%]: Loss: 1.1223\n",
      "Test(4): Loss: 1.5171, Accuracy: 38.2613%\n",
      "Train(5)[0%]: Loss: 1.0248\n",
      "Train(5)[50%]: Loss: 1.0472\n",
      "Test(5): Loss: 1.5243, Accuracy: 38.2093%\n",
      "Train(6)[0%]: Loss: 1.0021\n",
      "Train(6)[50%]: Loss: 1.0134\n",
      "Test(6): Loss: 1.5123, Accuracy: 39.1983%\n",
      "Train(7)[0%]: Loss: 0.9718\n",
      "Train(7)[50%]: Loss: 0.9779\n",
      "Test(7): Loss: 1.5103, Accuracy: 38.8600%\n",
      "Train(8)[0%]: Loss: 0.9529\n",
      "Train(8)[50%]: Loss: 0.9638\n",
      "Test(8): Loss: 1.5167, Accuracy: 37.4805%\n",
      "Train(9)[0%]: Loss: 0.9647\n",
      "Train(9)[50%]: Loss: 0.9485\n",
      "Test(9): Loss: 1.5163, Accuracy: 37.6106%\n",
      "Train(10)[0%]: Loss: 0.9674\n",
      "Train(10)[50%]: Loss: 0.9507\n",
      "Test(10): Loss: 1.5053, Accuracy: 38.6257%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_mfcc.shape[1]*train_mfcc.shape[2]\n",
    "out_channels = 5\n",
    "model = feed_forward(in_channels, out_channels) #.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6025\n",
      "Train(1)[50%]: Loss: 1.5330\n",
      "Test(1): Loss: 1.4530, Accuracy: 51.0672%\n",
      "Train(2)[0%]: Loss: 1.4233\n",
      "Train(2)[50%]: Loss: 1.3732\n",
      "Test(2): Loss: 1.4282, Accuracy: 54.2166%\n",
      "Train(3)[0%]: Loss: 1.3413\n",
      "Train(3)[50%]: Loss: 1.2978\n",
      "Test(3): Loss: 1.4115, Accuracy: 52.0302%\n",
      "Train(4)[0%]: Loss: 1.1278\n",
      "Train(4)[50%]: Loss: 1.2315\n",
      "Test(4): Loss: 1.3595, Accuracy: 57.4180%\n",
      "Train(5)[0%]: Loss: 1.1386\n",
      "Train(5)[50%]: Loss: 1.1937\n",
      "Test(5): Loss: 1.3981, Accuracy: 52.0042%\n",
      "Train(6)[0%]: Loss: 1.1493\n",
      "Train(6)[50%]: Loss: 1.1483\n",
      "Test(6): Loss: 1.3498, Accuracy: 56.4029%\n",
      "Train(7)[0%]: Loss: 1.1657\n",
      "Train(7)[50%]: Loss: 1.1282\n",
      "Test(7): Loss: 1.3452, Accuracy: 57.0016%\n",
      "Train(8)[0%]: Loss: 1.1316\n",
      "Train(8)[50%]: Loss: 1.1012\n",
      "Test(8): Loss: 1.3770, Accuracy: 52.9932%\n",
      "Train(9)[0%]: Loss: 1.0526\n",
      "Train(9)[50%]: Loss: 1.1038\n",
      "Test(9): Loss: 1.3757, Accuracy: 53.5659%\n",
      "Train(10)[0%]: Loss: 1.1543\n",
      "Train(10)[50%]: Loss: 1.0709\n",
      "Test(10): Loss: 1.3354, Accuracy: 57.8605%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader_mfcc, criterion, optimizer, epoch)\n",
    "    test(model, test_loader_mfcc, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms - Subset\n",
    "\n",
    "- Train Model on each subset of data at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-1.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-1.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-1.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-1.npy\")\n",
    "\n",
    "## Load new data\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-2.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-2.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-2.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-2.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2147, 64, 431)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup model\n",
    "\n",
    "in_channels = train_stft.shape[1]*train_stft.shape[2]\n",
    "out_channels = 5\n",
    "model = feed_forward(in_channels, out_channels) #.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.6184\n",
      "Train(1)[50%]: Loss: 1.5944\n",
      "Test(1): Loss: 1.5633, Accuracy: 41.7425%\n",
      "Train(2)[0%]: Loss: 1.5522\n",
      "Train(2)[50%]: Loss: 1.4922\n",
      "Test(2): Loss: 1.5189, Accuracy: 45.7737%\n",
      "Train(3)[0%]: Loss: 1.4186\n",
      "Train(3)[50%]: Loss: 1.4116\n",
      "Test(3): Loss: 1.4997, Accuracy: 48.1795%\n",
      "Train(4)[0%]: Loss: 1.2587\n",
      "Train(4)[50%]: Loss: 1.3359\n",
      "Test(4): Loss: 1.4798, Accuracy: 48.5696%\n",
      "Train(5)[0%]: Loss: 1.2270\n",
      "Train(5)[50%]: Loss: 1.2904\n",
      "Test(5): Loss: 1.4846, Accuracy: 48.0494%\n",
      "Train(6)[0%]: Loss: 1.2158\n",
      "Train(6)[50%]: Loss: 1.2289\n",
      "Test(6): Loss: 1.4712, Accuracy: 48.7646%\n",
      "Train(7)[0%]: Loss: 1.2271\n",
      "Train(7)[50%]: Loss: 1.1948\n",
      "Test(7): Loss: 1.4633, Accuracy: 48.5046%\n",
      "Train(8)[0%]: Loss: 1.2182\n",
      "Train(8)[50%]: Loss: 1.1543\n",
      "Test(8): Loss: 1.4723, Accuracy: 46.6840%\n",
      "Train(9)[0%]: Loss: 1.1040\n",
      "Train(9)[50%]: Loss: 1.1194\n",
      "Test(9): Loss: 1.4873, Accuracy: 45.4486%\n",
      "Train(10)[0%]: Loss: 1.1376\n",
      "Train(10)[50%]: Loss: 1.1031\n",
      "Test(10): Loss: 1.4543, Accuracy: 48.6346%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.2863\n",
      "Train(1)[50%]: Loss: 1.3045\n",
      "Test(1): Loss: 1.3974, Accuracy: 51.4974%\n",
      "Train(2)[0%]: Loss: 1.1357\n",
      "Train(2)[50%]: Loss: 1.2374\n",
      "Test(2): Loss: 1.3703, Accuracy: 54.8177%\n",
      "Train(3)[0%]: Loss: 1.1405\n",
      "Train(3)[50%]: Loss: 1.1600\n",
      "Test(3): Loss: 1.3731, Accuracy: 53.1901%\n",
      "Train(4)[0%]: Loss: 1.2903\n",
      "Train(4)[50%]: Loss: 1.1382\n",
      "Test(4): Loss: 1.3779, Accuracy: 52.2786%\n",
      "Train(5)[0%]: Loss: 1.1144\n",
      "Train(5)[50%]: Loss: 1.1071\n",
      "Test(5): Loss: 1.3816, Accuracy: 52.0833%\n",
      "Train(6)[0%]: Loss: 1.0812\n",
      "Train(6)[50%]: Loss: 1.0647\n",
      "Test(6): Loss: 1.3643, Accuracy: 53.7109%\n",
      "Train(7)[0%]: Loss: 1.0875\n",
      "Train(7)[50%]: Loss: 1.0561\n",
      "Test(7): Loss: 1.3882, Accuracy: 51.7578%\n",
      "Train(8)[0%]: Loss: 1.0247\n",
      "Train(8)[50%]: Loss: 1.0291\n",
      "Test(8): Loss: 1.3700, Accuracy: 53.9714%\n",
      "Train(9)[0%]: Loss: 1.0430\n",
      "Train(9)[50%]: Loss: 1.0250\n",
      "Test(9): Loss: 1.3699, Accuracy: 53.5156%\n",
      "Train(10)[0%]: Loss: 1.0294\n",
      "Train(10)[50%]: Loss: 1.0247\n",
      "Test(10): Loss: 1.3955, Accuracy: 50.9766%\n"
     ]
    }
   ],
   "source": [
    "## Load new data\n",
    "train_stft1 = np.load(\"Data/train_stft-dB-3.npy\", allow_pickle = True)\n",
    "test_stft1 = np.load(\"Data/test_stft-dB-3.npy\")\n",
    "train_labels1 = np.load(\"Data/train_labels_stft-dB-3.npy\")\n",
    "test_labels1 = np.load(\"Data/test_labels_stft-dB-3.npy\")\n",
    "\n",
    "train_stft2 = np.load(\"Data/train_stft-dB-4.npy\", allow_pickle = True)\n",
    "test_stft2 = np.load(\"Data/test_stft-dB-4.npy\")\n",
    "train_labels2 = np.load(\"Data/train_labels_stft-dB-4.npy\")\n",
    "test_labels2 = np.load(\"Data/test_labels_stft-dB-4.npy\")\n",
    "\n",
    "train_stft = np.concatenate((train_stft1, train_stft2))\n",
    "test_stft = np.concatenate((test_stft1, test_stft2))\n",
    "train_labels = np.concatenate((train_labels1, train_labels2))\n",
    "test_labels = np.concatenate((test_labels1, test_labels2))\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(1)[0%]: Loss: 1.1574\n",
      "Train(1)[50%]: Loss: 1.2358\n",
      "Test(1): Loss: 1.3981, Accuracy: 51.8229%\n",
      "Train(2)[0%]: Loss: 1.1395\n",
      "Train(2)[50%]: Loss: 1.1886\n",
      "Test(2): Loss: 1.3840, Accuracy: 52.4740%\n",
      "Train(3)[0%]: Loss: 1.1853\n",
      "Train(3)[50%]: Loss: 1.1411\n",
      "Test(3): Loss: 1.3792, Accuracy: 51.3021%\n",
      "Train(4)[0%]: Loss: 1.0862\n",
      "Train(4)[50%]: Loss: 1.0964\n",
      "Test(4): Loss: 1.3720, Accuracy: 54.2969%\n",
      "Train(5)[0%]: Loss: 1.0977\n",
      "Train(5)[50%]: Loss: 1.0691\n",
      "Test(5): Loss: 1.3704, Accuracy: 54.8177%\n",
      "Train(6)[0%]: Loss: 1.1407\n",
      "Train(6)[50%]: Loss: 1.0408\n",
      "Test(6): Loss: 1.3792, Accuracy: 51.6927%\n",
      "Train(7)[0%]: Loss: 1.0452\n",
      "Train(7)[50%]: Loss: 1.0188\n",
      "Test(7): Loss: 1.3969, Accuracy: 49.8698%\n",
      "Train(8)[0%]: Loss: 1.0318\n",
      "Train(8)[50%]: Loss: 1.0111\n",
      "Test(8): Loss: 1.3932, Accuracy: 50.0000%\n",
      "Train(9)[0%]: Loss: 1.0377\n",
      "Train(9)[50%]: Loss: 1.0000\n",
      "Test(9): Loss: 1.3718, Accuracy: 52.9948%\n",
      "Train(10)[0%]: Loss: 0.9355\n",
      "Train(10)[50%]: Loss: 0.9885\n",
      "Test(10): Loss: 1.3850, Accuracy: 52.3438%\n"
     ]
    }
   ],
   "source": [
    "## Load new data\n",
    "train_stft = np.load(\"Data/train_stft-dB-5.npy\", allow_pickle = True)\n",
    "test_stft = np.load(\"Data/test_stft-dB-5.npy\")\n",
    "train_labels = np.load(\"Data/train_labels_stft-dB-5.npy\")\n",
    "test_labels = np.load(\"Data/test_labels_stft-dB-5.npy\")\n",
    "\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_set = MyDataset(train_stft, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_set = MyDataset(test_stft, test_labels)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    test(model, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
